\relax 
\bibstyle{opticajnl}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Schrödinger_1935}
\citation{PhysRev.47.777}
\citation{RevModPhys.81.865}
\citation{Torlai2018}
\citation{LeCun2015}
\citation{Vaswani2017}
\citation{transformer_quantum_2022,doi:10.1126/sciadv.add7131}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Quantum Framework and Data Generation}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Neural Network Architectures}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Multi-Layer Perceptron (MLP)}{3}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Convolutional Neural Network (CNN)}{3}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Transformer Network}{4}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Classical Estimation Methods}{5}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Maximum Likelihood Estimation (MLE)}{5}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Bayesian Estimation}{6}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Training and Evaluation Framework}{6}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Inference and Evaluation}{7}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Results and Discussion}{8}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Accuracy Analysis}{8}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Computational Efficiency}{8}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Measurement Utilization}{8}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Mean squared error versus number of measurements for all methods. The dashed line shows theoretical $1/\sqrt  {N}$ scaling. Neural networks maintain optimal scaling behavior while requiring significantly less computation time than traditional approaches.}}{9}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mse_vs_measurements}{{1}{9}{Mean squared error versus number of measurements for all methods. The dashed line shows theoretical $1/\sqrt {N}$ scaling. Neural networks maintain optimal scaling behavior while requiring significantly less computation time than traditional approaches}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Time complexity analysis showing computation time scaling with number of measurements. Neural networks demonstrate near-linear scaling, while traditional methods show quadratic behavior. Reference lines indicate theoretical $O(N)$ and $O(N^2)$ scaling.}}{10}{figure.caption.2}\protected@file@percent }
\newlabel{fig:time_complexity}{{2}{10}{Time complexity analysis showing computation time scaling with number of measurements. Neural networks demonstrate near-linear scaling, while traditional methods show quadratic behavior. Reference lines indicate theoretical $O(N)$ and $O(N^2)$ scaling}{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Efficiency frontier comparing mean squared error (MSE) versus computation time for different methods. Points show results for measurement counts from 10 to 400. Lower-left region represents better performance (lower error, faster computation).}}{11}{figure.caption.3}\protected@file@percent }
\newlabel{fig:efficiency}{{3}{11}{Efficiency frontier comparing mean squared error (MSE) versus computation time for different methods. Points show results for measurement counts from 10 to 400. Lower-left region represents better performance (lower error, faster computation)}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Normalized improvement in accuracy with increased measurements, relative to baseline performance at 10 measurements. Neural networks show more efficient use of additional measurements, achieving better improvement factors than traditional methods. Reference lines show theoretical $\sqrt  {N}$ and $N$ scaling.}}{12}{figure.caption.4}\protected@file@percent }
\newlabel{fig:improvement}{{4}{12}{Normalized improvement in accuracy with increased measurements, relative to baseline performance at 10 measurements. Neural networks show more efficient use of additional measurements, achieving better improvement factors than traditional methods. Reference lines show theoretical $\sqrt {N}$ and $N$ scaling}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{12}{section.4}\protected@file@percent }
\bibdata{sample}
\bibcite{Schrödinger_1935}{1}
\bibcite{PhysRev.47.777}{2}
\bibcite{RevModPhys.81.865}{3}
\bibcite{Torlai2018}{4}
\bibcite{LeCun2015}{5}
\bibcite{Vaswani2017}{6}
\bibcite{transformer_quantum_2022}{7}
\bibcite{doi:10.1126/sciadv.add7131}{8}
\newlabel{LastPage}{{4}{13}{Data availability}{page.13}{}}
\gdef\lastpage@lastpage{13}
\gdef\lastpage@lastpageHy{13}
\gdef \@abspage@last{13}
